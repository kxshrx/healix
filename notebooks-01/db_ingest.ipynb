{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2599d025",
   "metadata": {},
   "source": [
    "# Healthcare Claims Database Ingestion\n",
    "\n",
    "This notebook ingests the cleaned healthcare dataset into a SQLite database with a single `claims_table` for efficient storage and querying of healthcare claims data.\n",
    "\n",
    "## Process Overview:\n",
    "1. **Data Loading** - Load cleaned CSV from PHA scrubber\n",
    "2. **Database Creation** - Create SQLite database with claims_table schema\n",
    "3. **Claims Ingestion** - Bulk insert all healthcare claims with patient hashing\n",
    "4. **Performance Optimization** - Create indexes for efficient querying\n",
    "5. **Validation & Reporting** - Verify data integrity and generate summary report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a626f207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthcare Claims Database Ingestion\n",
      "========================================\n",
      "Project root: /Users/kxshrx/asylum/healix\n",
      "Database will be created at: /Users/kxshrx/asylum/healix/db.sqlite\n",
      "Reports directory: /Users/kxshrx/asylum/healix\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get project root directory (parent of notebooks-01)\n",
    "project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "\n",
    "# Create outputs directory for reports and other files\n",
    "outputs_dir = project_root / \"\"\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Healthcare Claims Database Ingestion\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Database will be created at: {project_root}/db.sqlite\")\n",
    "print(f\"Reports directory: {outputs_dir.absolute()}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4deadb",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Validation\n",
    "\n",
    "Load the cleaned healthcare dataset and perform initial validation to ensure data quality before database ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee5e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned claims dataset...\n",
      "Using original file: post-eda.csv\n",
      "Dataset loaded successfully: 55500 records, 7 columns\n",
      "\n",
      "Columns in dataset:\n",
      "1. Age\n",
      "2. Gender\n",
      "3. Medical Condition\n",
      "4. Admission Type\n",
      "5. Insurance Provider\n",
      "6. Billing Amount\n",
      "7. length_of_stay_days\n",
      "\n",
      "Dataset info:\n",
      "Memory usage: 13.2 MB\n",
      "Missing values: 0\n",
      "\n",
      "Dataset ready for database ingestion\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset \n",
    "print(\"Loading cleaned claims dataset...\")\n",
    "\n",
    "try:\n",
    "    # Check if processed dataset exists (from PHA scrubber)\n",
    "    processed_files = list(project_root.glob(\"outputs/cleaned_healthcare_*\"))\n",
    "    if processed_files:\n",
    "        # Use the most recent processed file\n",
    "        claims_file = max(processed_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"Using processed file: {claims_file.name}\")\n",
    "    else:\n",
    "        # Fall back to original file\n",
    "        claims_file = project_root / \"outputs/post-eda.csv\"\n",
    "        print(f\"Using original file: {claims_file.name}\")\n",
    "    \n",
    "    df_claims = pd.read_csv(claims_file)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully: {df_claims.shape[0]} records, {df_claims.shape[1]} columns\")\n",
    "    print(\"\\nColumns in dataset:\")\n",
    "    for i, col in enumerate(df_claims.columns, 1):\n",
    "        print(f\"{i}. {col}\")\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"Memory usage: {df_claims.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"Missing values: {df_claims.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Expected columns for 7-column structure\n",
    "    expected_columns = ['Age', 'Gender', 'Medical Condition', 'Admission Type', \n",
    "                       'Insurance Provider', 'Billing Amount', 'length_of_stay_days']\n",
    "    \n",
    "    # Verify we have the expected columns\n",
    "    missing_cols = set(expected_columns) - set(df_claims.columns)\n",
    "    extra_cols = set(df_claims.columns) - set(expected_columns)\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nWarning: Missing expected columns: {missing_cols}\")\n",
    "    if extra_cols:\n",
    "        print(f\"\\nNote: Additional columns found: {extra_cols}\")\n",
    "    \n",
    "    print(f\"\\nDataset ready for database ingestion\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb110ec7",
   "metadata": {},
   "source": [
    "## 2. Database Schema Creation\n",
    "\n",
    "Create SQLite database with the claims_table schema optimized for healthcare claims storage and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75776bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database schema...\n",
      "Creating database: db.sqlite\n",
      "Database location: /Users/kxshrx/asylum/healix/db.sqlite\n",
      "Note: Database will be overridden if it exists\n",
      "Database tables created successfully\n",
      "   - healthcare_claims (main data table)\n",
      "   - ingestion_metadata (tracking table)\n",
      "   - Location: /Users/kxshrx/asylum/healix/db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Database Setup - Create tables with updated schema\n",
    "print(\"Setting up database schema...\")\n",
    "\n",
    "# Create database in project root directory (simple name, always override)\n",
    "db_name = \"db.sqlite\"\n",
    "db_path = project_root / db_name\n",
    "\n",
    "print(f\"Creating database: {db_name}\")\n",
    "print(f\"Database location: {db_path}\")\n",
    "print(\"Note: Database will be overridden if it exists\")\n",
    "\n",
    "try:\n",
    "    # Remove existing database if it exists\n",
    "    if db_path.exists():\n",
    "        db_path.unlink()\n",
    "        print(\"Existing database removed\")\n",
    "        \n",
    "    # Connect to SQLite database\n",
    "    db_conn = sqlite3.connect(str(db_path))\n",
    "    cursor = db_conn.cursor()\n",
    "    \n",
    "    # Create healthcare_claims table with updated 7-column schema\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS healthcare_claims (\n",
    "        claim_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        age INTEGER NOT NULL,\n",
    "        gender TEXT NOT NULL,\n",
    "        medical_condition TEXT NOT NULL,\n",
    "        admission_type TEXT NOT NULL,\n",
    "        insurance_provider TEXT NOT NULL,\n",
    "        billing_amount DECIMAL(10,2) NOT NULL,\n",
    "        length_of_stay_days INTEGER NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        \n",
    "        -- Data quality constraints\n",
    "        CHECK (age >= 0 AND age <= 120),\n",
    "        CHECK (gender IN ('Male', 'Female')),\n",
    "        CHECK (admission_type IN ('Emergency', 'Elective', 'Urgent')),\n",
    "        CHECK (billing_amount >= 0),\n",
    "        CHECK (length_of_stay_days >= 0)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(create_table_sql)\n",
    "    \n",
    "    # Create metadata table for tracking ingestion\n",
    "    metadata_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ingestion_metadata (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        source_file TEXT NOT NULL,\n",
    "        records_ingested INTEGER NOT NULL,\n",
    "        ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        data_version TEXT,\n",
    "        notes TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(metadata_sql)\n",
    "    \n",
    "    db_conn.commit()\n",
    "    print(\"Database tables created successfully\")\n",
    "    print(\"   - healthcare_claims (main data table)\")\n",
    "    print(\"   - ingestion_metadata (tracking table)\")\n",
    "    print(f\"   - Location: {db_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")\n",
    "    if 'db_conn' in locals():\n",
    "        db_conn.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93e72e",
   "metadata": {},
   "source": [
    "## 3. Claims Data Ingestion\n",
    "\n",
    "Generate anonymous patient hashes and bulk insert all healthcare claims into the database using parameterized queries for security and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccc0eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data ingestion...\n",
      "Inserting 55500 records...\n",
      "Applied 108 billing amount corrections (negative → positive)\n",
      "✅ Data ingestion completed successfully\n",
      "   - Records processed: 55500\n",
      "   - Records inserted: 55500\n",
      "   - Processing time: 1.09 seconds\n",
      "   - Data corrections: 108 (negative billing amounts fixed)\n",
      "Inserting 55500 records...\n",
      "Applied 108 billing amount corrections (negative → positive)\n",
      "✅ Data ingestion completed successfully\n",
      "   - Records processed: 55500\n",
      "   - Records inserted: 55500\n",
      "   - Processing time: 1.09 seconds\n",
      "   - Data corrections: 108 (negative billing amounts fixed)\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion - Insert claims data\n",
    "print(\"Starting data ingestion...\")\n",
    "\n",
    "ingestion_stats = {\n",
    "    'total_records': len(df_claims),\n",
    "    'processed_records': 0,\n",
    "    'errors': [],\n",
    "    'data_corrections': 0,\n",
    "    'start_time': datetime.now()\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Prepare column mapping for the 7-column structure\n",
    "    column_mapping = {\n",
    "        'Age': 'age',\n",
    "        'Gender': 'gender', \n",
    "        'Medical Condition': 'medical_condition',\n",
    "        'Admission Type': 'admission_type',\n",
    "        'Insurance Provider': 'insurance_provider',\n",
    "        'Billing Amount': 'billing_amount',\n",
    "        'length_of_stay_days': 'length_of_stay_days'\n",
    "    }\n",
    "    \n",
    "    # Verify all required columns exist\n",
    "    missing_columns = [col for col in column_mapping.keys() if col not in df_claims.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Prepare data for insertion with data quality corrections\n",
    "    records_to_insert = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        try:\n",
    "            # Fix negative billing amounts by taking absolute value\n",
    "            billing_amount = float(row['Billing Amount'])\n",
    "            if billing_amount < 0:\n",
    "                billing_amount = abs(billing_amount)\n",
    "                ingestion_stats['data_corrections'] += 1\n",
    "            \n",
    "            record = (\n",
    "                int(row['Age']),\n",
    "                str(row['Gender']).strip(),\n",
    "                str(row['Medical Condition']).strip(),\n",
    "                str(row['Admission Type']).strip(),\n",
    "                str(row['Insurance Provider']).strip(),\n",
    "                billing_amount,\n",
    "                int(row['length_of_stay_days'])\n",
    "            )\n",
    "            records_to_insert.append(record)\n",
    "            ingestion_stats['processed_records'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Row {idx}: {str(e)}\"\n",
    "            ingestion_stats['errors'].append(error_msg)\n",
    "            if len(ingestion_stats['errors']) > 10:  # Limit error reporting\n",
    "                break\n",
    "    \n",
    "    # Batch insert for better performance\n",
    "    print(f\"Inserting {len(records_to_insert)} records...\")\n",
    "    if ingestion_stats['data_corrections'] > 0:\n",
    "        print(f\"Applied {ingestion_stats['data_corrections']} billing amount corrections (negative → positive)\")\n",
    "    \n",
    "    insert_sql = \"\"\"\n",
    "    INSERT INTO healthcare_claims \n",
    "    (age, gender, medical_condition, admission_type, insurance_provider, billing_amount, length_of_stay_days)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.executemany(insert_sql, records_to_insert)\n",
    "    \n",
    "    # Record ingestion metadata\n",
    "    metadata_insert = \"\"\"\n",
    "    INSERT INTO ingestion_metadata \n",
    "    (source_file, records_ingested, data_version, notes)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "    \n",
    "    source_file_name = claims_file.name if 'claims_file' in locals() else 'unknown'\n",
    "    notes = f\"7-column structure ingestion. Errors: {len(ingestion_stats['errors'])}, Corrections: {ingestion_stats['data_corrections']}\"\n",
    "    \n",
    "    cursor.execute(metadata_insert, (\n",
    "        source_file_name,\n",
    "        len(records_to_insert),\n",
    "        \"v2.0_7columns\",\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    db_conn.commit()\n",
    "    ingestion_stats['end_time'] = datetime.now()\n",
    "    ingestion_stats['duration'] = (ingestion_stats['end_time'] - ingestion_stats['start_time']).total_seconds()\n",
    "    \n",
    "    print(\"✅ Data ingestion completed successfully\")\n",
    "    print(f\"   - Records processed: {ingestion_stats['processed_records']}\")\n",
    "    print(f\"   - Records inserted: {len(records_to_insert)}\")\n",
    "    print(f\"   - Processing time: {ingestion_stats['duration']:.2f} seconds\")\n",
    "    print(f\"   - Data corrections: {ingestion_stats['data_corrections']} (negative billing amounts fixed)\")\n",
    "    \n",
    "    if ingestion_stats['errors']:\n",
    "        print(f\"   - Errors encountered: {len(ingestion_stats['errors'])}\")\n",
    "        print(\"   - First few errors:\")\n",
    "        for error in ingestion_stats['errors'][:3]:\n",
    "            print(f\"     • {error}\")\n",
    "    \n",
    "    ingestion_complete = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during ingestion: {e}\")\n",
    "    ingestion_complete = False\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be5dd6",
   "metadata": {},
   "source": [
    "## 4. Performance Optimization\n",
    "\n",
    "Create database indexes on frequently queried columns to improve query performance for analytics and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e96388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating database indexes for optimal query performance...\n",
      "✅ Database indexing completed\n",
      "   - Indexes created: 11\n",
      "   - Performance optimization: Ready for analytics queries\n",
      "\n",
      "All indexes in database:\n",
      "   1. ✅ idx_insurance_provider\n",
      "   2. ✅ idx_medical_condition\n",
      "   3. ✅ idx_billing_amount\n",
      "   4. ✅ idx_admission_type\n",
      "   5. ✅ idx_age\n",
      "   6. ✅ idx_gender\n",
      "   7. ✅ idx_length_stay\n",
      "   8. ✅ idx_provider_condition\n",
      "   9. ✅ idx_age_gender\n",
      "   10. ✅ idx_admission_billing\n",
      "   11. ✅ idx_created_at\n",
      "✅ Database indexing completed\n",
      "   - Indexes created: 11\n",
      "   - Performance optimization: Ready for analytics queries\n",
      "\n",
      "All indexes in database:\n",
      "   1. ✅ idx_insurance_provider\n",
      "   2. ✅ idx_medical_condition\n",
      "   3. ✅ idx_billing_amount\n",
      "   4. ✅ idx_admission_type\n",
      "   5. ✅ idx_age\n",
      "   6. ✅ idx_gender\n",
      "   7. ✅ idx_length_stay\n",
      "   8. ✅ idx_provider_condition\n",
      "   9. ✅ idx_age_gender\n",
      "   10. ✅ idx_admission_billing\n",
      "   11. ✅ idx_created_at\n"
     ]
    }
   ],
   "source": [
    "# Create Database Indexes for Query Performance\n",
    "print(\"Creating database indexes for optimal query performance...\")\n",
    "\n",
    "indexes_to_create = [\n",
    "    # Core business indexes\n",
    "    (\"idx_insurance_provider\", \"healthcare_claims\", \"insurance_provider\"),\n",
    "    (\"idx_medical_condition\", \"healthcare_claims\", \"medical_condition\"),  \n",
    "    (\"idx_billing_amount\", \"healthcare_claims\", \"billing_amount\"),\n",
    "    (\"idx_admission_type\", \"healthcare_claims\", \"admission_type\"),\n",
    "    \n",
    "    # Demographic indexes\n",
    "    (\"idx_age\", \"healthcare_claims\", \"age\"),\n",
    "    (\"idx_gender\", \"healthcare_claims\", \"gender\"),\n",
    "    (\"idx_length_stay\", \"healthcare_claims\", \"length_of_stay_days\"),\n",
    "    \n",
    "    # Composite indexes for common queries\n",
    "    (\"idx_provider_condition\", \"healthcare_claims\", \"insurance_provider, medical_condition\"),\n",
    "    (\"idx_age_gender\", \"healthcare_claims\", \"age, gender\"),\n",
    "    (\"idx_admission_billing\", \"healthcare_claims\", \"admission_type, billing_amount\"),\n",
    "    \n",
    "    # Timestamp index\n",
    "    (\"idx_created_at\", \"healthcare_claims\", \"created_at\")\n",
    "]\n",
    "\n",
    "index_success = True\n",
    "created_indexes = []\n",
    "\n",
    "try:\n",
    "    for index_name, table_name, columns in indexes_to_create:\n",
    "        try:\n",
    "            create_index_sql = f\"CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} ({columns})\"\n",
    "            cursor.execute(create_index_sql)\n",
    "            created_indexes.append(index_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Warning: Could not create index {index_name}: {e}\")\n",
    "            index_success = False\n",
    "    \n",
    "    db_conn.commit()\n",
    "    \n",
    "    print(f\"✅ Database indexing completed\")\n",
    "    print(f\"   - Indexes created: {len(created_indexes)}\")\n",
    "    print(f\"   - Performance optimization: Ready for analytics queries\")\n",
    "    \n",
    "    # Verify indexes were created\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='index' AND sql IS NOT NULL\")\n",
    "    all_indexes = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nAll indexes in database:\")\n",
    "    for idx, (index_name,) in enumerate(all_indexes, 1):\n",
    "        status = \"✅\" if index_name in created_indexes else \"📋\"\n",
    "        print(f\"   {idx}. {status} {index_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating indexes: {e}\")\n",
    "    index_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990958f8",
   "metadata": {},
   "source": [
    "## 5. Data Validation and Sample Queries\n",
    "\n",
    "Verify database integrity and demonstrate query capabilities with sample analytics queries to ensure the data was ingested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72bf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing data validation and quality checks...\n",
      "✅ Validation Results:\n",
      "   📊 Records: 55500 ingested\n",
      "   🔍 Match source: ✅\n",
      "\n",
      "   📈 Data Distribution:\n",
      "      - Insurance providers: 5\n",
      "      - Medical conditions: 6\n",
      "      - Admission types: 3\n",
      "      - Age range: 13-89 years\n",
      "      - Billing range: $9.24-$52,764.28\n",
      "      - Stay range: 1-30 days\n",
      "\n",
      "   🔎 Data Quality:\n",
      "      - ✅ All data passed quality checks\n",
      "\n",
      "   💾 Sample verification: 5 records retrieved\n",
      "✅ Validation Results:\n",
      "   📊 Records: 55500 ingested\n",
      "   🔍 Match source: ✅\n",
      "\n",
      "   📈 Data Distribution:\n",
      "      - Insurance providers: 5\n",
      "      - Medical conditions: 6\n",
      "      - Admission types: 3\n",
      "      - Age range: 13-89 years\n",
      "      - Billing range: $9.24-$52,764.28\n",
      "      - Stay range: 1-30 days\n",
      "\n",
      "   🔎 Data Quality:\n",
      "      - ✅ All data passed quality checks\n",
      "\n",
      "   💾 Sample verification: 5 records retrieved\n"
     ]
    }
   ],
   "source": [
    "# Data Validation and Quality Checks\n",
    "print(\"Performing data validation and quality checks...\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "try:\n",
    "    # 1. Record counts validation\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM healthcare_claims\")\n",
    "    db_record_count = cursor.fetchone()[0]\n",
    "    \n",
    "    validation_results['record_count'] = {\n",
    "        'source_records': len(df_claims),\n",
    "        'database_records': db_record_count,\n",
    "        'match': db_record_count == len(df_claims)\n",
    "    }\n",
    "    \n",
    "    # 2. Data distribution validation  \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT insurance_provider) as unique_providers,\n",
    "            COUNT(DISTINCT medical_condition) as unique_conditions,\n",
    "            COUNT(DISTINCT admission_type) as unique_admission_types,\n",
    "            COUNT(DISTINCT gender) as unique_genders,\n",
    "            MIN(age) as min_age,\n",
    "            MAX(age) as max_age,\n",
    "            MIN(billing_amount) as min_billing,\n",
    "            MAX(billing_amount) as max_billing,\n",
    "            MIN(length_of_stay_days) as min_stay,\n",
    "            MAX(length_of_stay_days) as max_stay\n",
    "        FROM healthcare_claims\n",
    "    \"\"\")\n",
    "    \n",
    "    distribution_stats = cursor.fetchone()\n",
    "    validation_results['data_distribution'] = {\n",
    "        'unique_providers': distribution_stats[0],\n",
    "        'unique_conditions': distribution_stats[1], \n",
    "        'unique_admission_types': distribution_stats[2],\n",
    "        'unique_genders': distribution_stats[3],\n",
    "        'age_range': (distribution_stats[4], distribution_stats[5]),\n",
    "        'billing_range': (distribution_stats[6], distribution_stats[7]),\n",
    "        'stay_range': (distribution_stats[8], distribution_stats[9])\n",
    "    }\n",
    "    \n",
    "    # 3. Data quality checks\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN age IS NULL OR age < 0 OR age > 120 THEN 1 ELSE 0 END) as invalid_age,\n",
    "            SUM(CASE WHEN gender IS NULL OR TRIM(gender) = '' THEN 1 ELSE 0 END) as invalid_gender,\n",
    "            SUM(CASE WHEN medical_condition IS NULL OR TRIM(medical_condition) = '' THEN 1 ELSE 0 END) as invalid_condition,\n",
    "            SUM(CASE WHEN billing_amount IS NULL OR billing_amount < 0 THEN 1 ELSE 0 END) as invalid_billing,\n",
    "            SUM(CASE WHEN length_of_stay_days IS NULL OR length_of_stay_days < 0 THEN 1 ELSE 0 END) as invalid_stay\n",
    "        FROM healthcare_claims\n",
    "    \"\"\")\n",
    "    \n",
    "    quality_stats = cursor.fetchone()\n",
    "    validation_results['data_quality'] = {\n",
    "        'invalid_age': quality_stats[0],\n",
    "        'invalid_gender': quality_stats[1],\n",
    "        'invalid_condition': quality_stats[2], \n",
    "        'invalid_billing': quality_stats[3],\n",
    "        'invalid_stay': quality_stats[4]\n",
    "    }\n",
    "    \n",
    "    # 4. Sample data verification\n",
    "    cursor.execute(\"SELECT * FROM healthcare_claims LIMIT 5\")\n",
    "    sample_records = cursor.fetchall()\n",
    "    validation_results['sample_data'] = len(sample_records)\n",
    "    \n",
    "    # Print validation summary\n",
    "    print(\"✅ Validation Results:\")\n",
    "    print(f\"   📊 Records: {validation_results['record_count']['database_records']} ingested\")\n",
    "    print(f\"   🔍 Match source: {'✅' if validation_results['record_count']['match'] else '❌'}\")\n",
    "    \n",
    "    print(f\"\\n   📈 Data Distribution:\")\n",
    "    dist = validation_results['data_distribution']\n",
    "    print(f\"      - Insurance providers: {dist['unique_providers']}\")\n",
    "    print(f\"      - Medical conditions: {dist['unique_conditions']}\")\n",
    "    print(f\"      - Admission types: {dist['unique_admission_types']}\")  \n",
    "    print(f\"      - Age range: {dist['age_range'][0]}-{dist['age_range'][1]} years\")\n",
    "    print(f\"      - Billing range: ${dist['billing_range'][0]:,.2f}-${dist['billing_range'][1]:,.2f}\")\n",
    "    print(f\"      - Stay range: {dist['stay_range'][0]}-{dist['stay_range'][1]} days\")\n",
    "    \n",
    "    quality = validation_results['data_quality']\n",
    "    total_quality_issues = sum(quality.values())\n",
    "    \n",
    "    print(f\"\\n   🔎 Data Quality:\")\n",
    "    if total_quality_issues == 0:\n",
    "        print(\"      - ✅ All data passed quality checks\")\n",
    "    else:\n",
    "        print(f\"      - ⚠️  Total quality issues: {total_quality_issues}\")\n",
    "        for issue_type, count in quality.items():\n",
    "            if count > 0:\n",
    "                print(f\"        • {issue_type}: {count} records\")\n",
    "    \n",
    "    print(f\"\\n   💾 Sample verification: {validation_results['sample_data']} records retrieved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during validation: {e}\")\n",
    "    validation_results['error'] = str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2664a",
   "metadata": {},
   "source": [
    "## 6. Generate Ingestion Report\n",
    "\n",
    "Create a comprehensive report summarizing the database ingestion process, statistics, and any warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6244aca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating database ingestion report...\n",
      "✅ Database ingestion completed successfully!\n",
      "\n",
      "📊 FINAL SUMMARY:\n",
      "   - Database: db.sqlite\n",
      "   - Location: /Users/kxshrx/asylum/healix/db.sqlite\n",
      "   - Records: 55,500\n",
      "   - Size: 14.30 MB\n",
      "   - Report: db_ingestion_report_20250929_161354.txt (in outputs/)\n",
      "\n",
      "🔗 Easy Access:\n",
      "   - Database path: /Users/kxshrx/asylum/healix/db.sqlite\n",
      "   - CLI connect: sqlite3 /Users/kxshrx/asylum/healix/db.sqlite\n",
      "\n",
      "✨ Database is ready for:\n",
      "   - Analytics queries\n",
      "   - Machine learning workflows\n",
      "   - Business intelligence tools\n",
      "   - Data visualization\n",
      "\n",
      "🔒 Database connection closed safely\n"
     ]
    }
   ],
   "source": [
    "# Generate Database Report and Finalize\n",
    "print(\"Generating database ingestion report...\")\n",
    "\n",
    "try:\n",
    "    # Get database file size\n",
    "    db_size_after = db_path.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report_content = f\"\"\"\n",
    "HEALTHCARE CLAIMS DATABASE INGESTION REPORT\n",
    "==========================================\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATABASE INFORMATION:\n",
    "- Database Name: {db_name}\n",
    "- Database Location: {db_path}\n",
    "- Database Size: {db_size_after:.2f} MB\n",
    "- Schema Version: 7-column structure (v2.0)\n",
    "\n",
    "SOURCE DATA:\n",
    "- Source File: {claims_file.name if 'claims_file' in locals() else 'Unknown'}\n",
    "- Records in Source: {ingestion_stats.get('total_records', 'Unknown')}\n",
    "- Processing Time: {ingestion_stats.get('duration', 0):.2f} seconds\n",
    "\n",
    "INGESTION RESULTS:\n",
    "- Records Successfully Processed: {ingestion_stats.get('processed_records', 0)}\n",
    "- Records Inserted to Database: {validation_results.get('record_count', {}).get('database_records', 0)}\n",
    "- Data Integrity Check: {'PASSED' if validation_results.get('record_count', {}).get('match', False) else 'FAILED'}\n",
    "- Processing Errors: {len(ingestion_stats.get('errors', []))}\n",
    "\n",
    "DATABASE SCHEMA:\n",
    "Table: healthcare_claims\n",
    "- claim_id (PRIMARY KEY)\n",
    "- age (INTEGER, 0-120)\n",
    "- gender (TEXT, Male/Female) \n",
    "- medical_condition (TEXT)\n",
    "- admission_type (TEXT, Emergency/Elective/Urgent)\n",
    "- insurance_provider (TEXT)\n",
    "- billing_amount (DECIMAL)\n",
    "- length_of_stay_days (INTEGER)\n",
    "- created_at (TIMESTAMP)\n",
    "\n",
    "INDEXES CREATED: {len(created_indexes) if 'created_indexes' in locals() else 0}\n",
    "- Performance optimization indexes for all key columns\n",
    "- Composite indexes for common query patterns\n",
    "\n",
    "DATA QUALITY SUMMARY:\n",
    "- Unique Insurance Providers: {validation_results.get('data_distribution', {}).get('unique_providers', 'N/A')}\n",
    "- Unique Medical Conditions: {validation_results.get('data_distribution', {}).get('unique_conditions', 'N/A')}  \n",
    "- Age Range: {validation_results.get('data_distribution', {}).get('age_range', (0,0))[0]}-{validation_results.get('data_distribution', {}).get('age_range', (0,0))[1]} years\n",
    "- Billing Amount Range: ${validation_results.get('data_distribution', {}).get('billing_range', (0,0))[0]:,.2f} - ${validation_results.get('data_distribution', {}).get('billing_range', (0,0))[1]:,.2f}\n",
    "- Length of Stay Range: {validation_results.get('data_distribution', {}).get('stay_range', (0,0))[0]}-{validation_results.get('data_distribution', {}).get('stay_range', (0,0))[1]} days\n",
    "\n",
    "QUICK ACCESS:\n",
    "- Database: {db_path}\n",
    "- Connect: sqlite3 {db_path}\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Use the database for analytics and machine learning\n",
    "2. Connect to BI tools for visualization\n",
    "3. Run queries for business insights\n",
    "4. Backup database regularly\n",
    "\n",
    "SAMPLE QUERIES:\n",
    "-- Provider analysis\n",
    "SELECT insurance_provider, COUNT(*), AVG(billing_amount) \n",
    "FROM healthcare_claims \n",
    "GROUP BY insurance_provider;\n",
    "\n",
    "-- Condition trends  \n",
    "SELECT medical_condition, AVG(length_of_stay_days), AVG(billing_amount)\n",
    "FROM healthcare_claims \n",
    "GROUP BY medical_condition\n",
    "ORDER BY AVG(billing_amount) DESC;\n",
    "\n",
    "-- Age demographics\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN age < 30 THEN 'Under 30'\n",
    "        WHEN age < 50 THEN '30-49' \n",
    "        WHEN age < 70 THEN '50-69'\n",
    "        ELSE '70+'\n",
    "    END as age_group,\n",
    "    COUNT(*) as patients,\n",
    "    AVG(billing_amount) as avg_cost\n",
    "FROM healthcare_claims \n",
    "GROUP BY age_group;\n",
    "\n",
    "DATABASE STATUS: READY FOR ANALYTICS\n",
    "=====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Save report to outputs directory\n",
    "    report_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_filename = f\"db_ingestion_report_{report_timestamp}.txt\"\n",
    "    report_path = outputs_dir / report_filename\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    print(\"✅ Database ingestion completed successfully!\")\n",
    "    print(f\"\\n📊 FINAL SUMMARY:\")\n",
    "    print(f\"   - Database: {db_name}\")\n",
    "    print(f\"   - Location: {db_path}\")\n",
    "    print(f\"   - Records: {validation_results.get('record_count', {}).get('database_records', 0):,}\")\n",
    "    print(f\"   - Size: {db_size_after:.2f} MB\")\n",
    "    print(f\"   - Report: {report_filename} (in outputs/)\")\n",
    "    \n",
    "    print(f\"\\n🔗 Easy Access:\")\n",
    "    print(f\"   - Database path: {db_path}\")\n",
    "    print(f\"   - CLI connect: sqlite3 {db_path}\")\n",
    "    \n",
    "    print(f\"\\n✨ Database is ready for:\")\n",
    "    print(f\"   - Analytics queries\")\n",
    "    print(f\"   - Machine learning workflows\") \n",
    "    print(f\"   - Business intelligence tools\")\n",
    "    print(f\"   - Data visualization\")\n",
    "    \n",
    "    report_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating report: {e}\")\n",
    "    report_success = False\n",
    "\n",
    "finally:\n",
    "    # Clean up database connection\n",
    "    if 'db_conn' in locals():\n",
    "        db_conn.close()\n",
    "        print(f\"\\n🔒 Database connection closed safely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53ba22",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Final Output Summary\n",
    "\n",
    "Close database connections and provide final summary of all generated files and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b7a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing database connectivity and sample queries...\n",
      "✅ Database connectivity test successful!\n",
      "📍 Database location: /Users/kxshrx/asylum/healix/db.sqlite\n",
      "\n",
      "📈 QUICK ANALYTICS PREVIEW:\n",
      "   Total Claims in Database: 55,500\n",
      "\n",
      "🏥 Top Insurance Providers:\n",
      "   • Cigna: 11249 claims, $25,527.96 avg, 15.5 days avg stay\n",
      "   • Medicare: 11154 claims, $25,617.86 avg, 15.6 days avg stay\n",
      "   • UnitedHealthcare: 11125 claims, $25,390.10 avg, 15.5 days avg stay\n",
      "   • Blue Cross: 11059 claims, $25,614.45 avg, 15.5 days avg stay\n",
      "   • Aetna: 10913 claims, $25,556.59 avg, 15.4 days avg stay\n",
      "\n",
      "🩺 Most Common Conditions:\n",
      "   • Arthritis: 9308 cases, $25,498.58 avg cost\n",
      "   • Diabetes: 9304 cases, $25,640.13 avg cost\n",
      "   • Hypertension: 9245 cases, $25,498.99 avg cost\n",
      "   • Obesity: 9231 cases, $25,808.22 avg cost\n",
      "   • Cancer: 9227 cases, $25,164.18 avg cost\n",
      "\n",
      "✨ Database is fully operational and ready for advanced analytics!\n",
      "💡 Connect via CLI: sqlite3 /Users/kxshrx/asylum/healix/db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Optional: Quick Database Query Test\n",
    "print(\"Testing database connectivity and sample queries...\")\n",
    "\n",
    "try:\n",
    "    # Reconnect for testing (using simple path)\n",
    "    test_conn = sqlite3.connect(str(db_path))\n",
    "    test_cursor = test_conn.cursor()\n",
    "    \n",
    "    # Test query 1: Basic count\n",
    "    test_cursor.execute(\"SELECT COUNT(*) as total_claims FROM healthcare_claims\")\n",
    "    total_count = test_cursor.fetchone()[0]\n",
    "    \n",
    "    # Test query 2: Provider summary\n",
    "    test_cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            insurance_provider,\n",
    "            COUNT(*) as claim_count,\n",
    "            ROUND(AVG(billing_amount), 2) as avg_billing,\n",
    "            ROUND(AVG(length_of_stay_days), 1) as avg_stay\n",
    "        FROM healthcare_claims \n",
    "        GROUP BY insurance_provider\n",
    "        ORDER BY claim_count DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    provider_summary = test_cursor.fetchall()\n",
    "    \n",
    "    # Test query 3: Top medical conditions\n",
    "    test_cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            medical_condition,\n",
    "            COUNT(*) as cases,\n",
    "            ROUND(AVG(billing_amount), 2) as avg_cost\n",
    "        FROM healthcare_claims \n",
    "        GROUP BY medical_condition\n",
    "        ORDER BY cases DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    condition_summary = test_cursor.fetchall()\n",
    "    \n",
    "    print(\"✅ Database connectivity test successful!\")\n",
    "    print(f\"📍 Database location: {db_path}\")\n",
    "    print(f\"\\n📈 QUICK ANALYTICS PREVIEW:\")\n",
    "    print(f\"   Total Claims in Database: {total_count:,}\")\n",
    "    \n",
    "    print(f\"\\n🏥 Top Insurance Providers:\")\n",
    "    for provider, count, avg_billing, avg_stay in provider_summary:\n",
    "        print(f\"   • {provider}: {count} claims, ${avg_billing:,.2f} avg, {avg_stay} days avg stay\")\n",
    "    \n",
    "    print(f\"\\n🩺 Most Common Conditions:\")\n",
    "    for condition, cases, avg_cost in condition_summary:\n",
    "        print(f\"   • {condition}: {cases} cases, ${avg_cost:,.2f} avg cost\")\n",
    "    \n",
    "    print(f\"\\n✨ Database is fully operational and ready for advanced analytics!\")\n",
    "    print(f\"💡 Connect via CLI: sqlite3 {db_path}\")\n",
    "    \n",
    "    test_conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Database test warning: {e}\")\n",
    "    print(\"   Database may still be functional - check connection manually\")\n",
    "    if 'test_conn' in locals():\n",
    "        test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73f3f7",
   "metadata": {},
   "source": [
    "## Final Output Summary\n",
    "\n",
    "### Generated Files:\n",
    "- **`db.sqlite`** - SQLite database with healthcare claims (in project root)\n",
    "- **`outputs/db_ingestion_report_*.txt`** - Comprehensive ingestion report with statistics and usage examples\n",
    "\n",
    "### Database Structure:\n",
    "- **`healthcare_claims`** - Streamlined healthcare claims table\n",
    "  - Core healthcare data with patient demographics and billing information\n",
    "  - Optimized indexes for efficient querying by provider, condition, and billing amount\n",
    "  - 55,500 records with 7 essential columns\n",
    "\n",
    "### Dataset Columns:\n",
    "1. **claim_id** - Primary key (auto-increment)\n",
    "2. **age** - Patient age (13-89 years)\n",
    "3. **gender** - Patient gender (Male/Female)\n",
    "4. **medical_condition** - Primary diagnosis (6 conditions)\n",
    "5. **admission_type** - Healthcare service type (Emergency/Urgent/Elective)\n",
    "6. **insurance_provider** - Insurance company (5 providers)\n",
    "7. **billing_amount** - Total cost of care ($9.24 - $52,764.28)\n",
    "8. **length_of_stay_days** - Duration of hospitalization (1-30 days)\n",
    "\n",
    "### Quick Access:\n",
    "- **Database location**: `db.sqlite` (project root)\n",
    "- **CLI connection**: `sqlite3 db.sqlite`\n",
    "- **Size**: 14.3 MB with full indexing\n",
    "\n",
    "### Usage Examples:\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the claims database\n",
    "conn = sqlite3.connect('db.sqlite')\n",
    "\n",
    "# Sample analytics queries\n",
    "# 1. Claims analysis by insurance provider\n",
    "provider_analysis = pd.read_sql_query(\"\"\"\n",
    "    SELECT insurance_provider, \n",
    "           COUNT(*) as claims, \n",
    "           AVG(billing_amount) as avg_billing,\n",
    "           AVG(length_of_stay_days) as avg_los\n",
    "    FROM healthcare_claims \n",
    "    GROUP BY insurance_provider \n",
    "    ORDER BY claims DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "# 2. Most expensive medical conditions\n",
    "condition_costs = pd.read_sql_query(\"\"\"\n",
    "    SELECT medical_condition, \n",
    "           COUNT(*) as cases, \n",
    "           AVG(billing_amount) as avg_cost\n",
    "    FROM healthcare_claims \n",
    "    GROUP BY medical_condition \n",
    "    ORDER BY avg_cost DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "# 3. Admission type cost analysis\n",
    "admission_analysis = pd.read_sql_query(\"\"\"\n",
    "    SELECT admission_type,\n",
    "           COUNT(*) as count,\n",
    "           AVG(billing_amount) as avg_cost,\n",
    "           AVG(length_of_stay_days) as avg_los\n",
    "    FROM healthcare_claims\n",
    "    GROUP BY admission_type\n",
    "\"\"\", conn)\n",
    "\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "**The healthcare claims database is now ready at `db.sqlite` for immediate use in analytics, machine learning, and business intelligence workflows!**\n",
    "\n",
    "Simply run `sqlite3 db.sqlite` from the project root to start querying the database directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
